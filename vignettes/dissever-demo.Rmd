---
title: "Dissever Tutorial"
author: "Malone, B. and Roudier, P."
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r libs, results='hide', message=FALSE}
library(dissever)

library(raster)
library(viridis)
library(dplyr)
library(magrittr)
```

Here is the coarse model to dissever:
```{r plot_coarse, fig.width=6, fig.height=6}
plot(edgeroi$carbon, col = viridis(100))
```
and here are the covariates that will be used for the disseveration:

```{r plot_fine, fig.width=8, fig.height=6}
plot(edgeroi$predictors, col = viridis(100))
```

## Example

In this section, we will dissever the sample dataset  with the available environmental covariates, using 4 different learning methods: 

* Random Forest (`"rf"`)
* Cubist (`"cubist"`)
* Generalized Additive Model using Splines (`"gamSpline"`)
* Linear Model (`"lm"`)
* Multivariate Adaptive Regression Spline (`"earth"`)

It is simple to switch between regression/classification methods using the `method` option of `dissever`.

But first let's setup some parameters:
```{r params}
min_iter <- 5 # Minimum number of iterations
max_iter <- 15 # Maximum number of iterations
p_train <- 0.1 # Subsampling of the initial data
```

We can then launch the different disseveration procedures:

```{r process, results='hide', message=FALSE}
# Random Forest
res_rf <- dissever(
    coarse = edgeroi$carbon, # coarse resolution raster
    fine = edgeroi$predictors, # stack of fine resolution covariates
    method = "rf", # regression method used for disseveration
    p = p_train, # proportion of pixels sampled for training regression model
    min_iter = min_iter, # minimum iterations
    max_iter = max_iter # maximum iterations
  )

# Cubist
res_cubist <- dissever(
  coarse = edgeroi$carbon, 
  fine = edgeroi$predictors, 
  method = "cubist", 
  p = p_train, 
  min_iter = min_iter,
  max_iter = max_iter
)

# GAM
res_gam <- dissever(
  coarse = edgeroi$carbon, 
  fine = edgeroi$predictors, 
  method = "gamSpline", 
  p = p_train, 
  min_iter = min_iter,
  max_iter = max_iter
)

# Linear model
res_lm <- dissever(
  coarse = edgeroi$carbon, 
  fine = edgeroi$predictors, 
  method = "lm", 
  p = p_train, 
  min_iter = min_iter,
  max_iter = max_iter
)

# MARS
res_earth <- dissever(
  coarse = edgeroi$carbon, 
  fine = edgeroi$predictors, 
  method = "earth", 
  p = p_train, 
  min_iter = min_iter,
  max_iter = max_iter
)

# Random Forest - example for downscaling categorical data
# First we reclassify the numeric raster into 4 classes
m <- cellStats(edgeroi$carbon, stat=fivenum) 
m <- matrix( c( m[1], m[2], 1, m[2], m[3], 2, m[3], m[4], 3, m[4], m[5], 4) , ncol=3, byrow=TRUE)                
edgeroi$categorical <- reclassify(edgeroi$carbon,m)
# We then apply the downscaling procedure
res_rf_cat <- dissever(
    coarse = edgeroi$categorical, # coarse resolution raster
    fine = edgeroi$predictors, # stack of fine resolution covariates
    method = "rf", # regression method used for disseveration
    p = p_train, # proportion of pixels sampled for training regression model
    min_iter = min_iter, # minimum iterations
    max_iter = max_iter, # maximum iterations
    data_type = "categorical"
)
```

# Analysing the results

The object returned by `dissever` is an object of class `dissever`. It's basically a `list` with 3 elements:
* `fit`: a `train` object storing the regression model used in the final disseveration
* `map`: a `RasterLayer` object storing the dissevered map
* `perf`: a `data.frame` object storing the evolution of the RMSE (and confidence intervals) with the disseveration iterations.

The `dissever` result has a `plot` function that can plot either the map or the performance results, depending on the `type` option.

## Maps

Below are the dissevered maps for the different methods:

```{r maps, fig.width=8, fig.height=8}
# Plotting maps
par(mfrow = c(2, 4))
plot(res_rf, type = 'map', main = "Random Forest")
plot(res_cubist, type = 'map', main = "Cubist")
plot(res_gam, type = 'map', main = "GAM")
plot(res_lm, type = 'map', main = "Linear Model")
plot(res_earth, type = 'map', main = "MARS")
plot(res_rf_cat, type = 'map', main = "Random Forest - Categorical Data")
```

## Convergence

We can also analyse and plot the optimisation of the dissevering model:

```{r optim, fig.width=8, fig.height=8}
# Plot performance
par(mfrow = c(2, 4))
plot(res_rf, type = 'perf', main = "Random Forest")
plot(res_cubist, type = 'perf', main = "Cubist")
plot(res_gam, type = 'perf', main = "GAM")
plot(res_lm, type = 'perf', main = "Linear Model")
plot(res_earth, type = 'perf', main = "MARS")
plot(res_rf_cat, type = 'perf', main = "Random Forest - Categorical Data")
```

## Performance of the regression 

The tools provided by the `caret` package can also be leveraged, e.g. to plot the observed vs. predicted values:

```{r preds_obs, fig.width=6, fig.height=4}
# Plot preds vs obs
preds <- extractPrediction(list(res_rf$fit, res_cubist$fit, res_gam$fit, res_lm$fit, res_earth$fit))
plotObsVsPred(preds)
```

The models can be compared using the `preds` object that we just derived using the `extractPrediction` function. In this case I'm computing the R^2^, the RMSE and the CCC for each model:

```{r compare}
# Compare models
perf <- preds %>% 
  group_by(model, dataType) %>% 
  summarise(
    rsq = cor(obs, pred)^2, 
    rmse = sqrt(mean((pred - obs)^2))
  )
perf
```

# Ensemble approach

We can either take the best option (in this case Random Forest -- but the results probably show that we should add some kind of validation process), or use a simple ensemble-type approach. For the latter, I am computing weights for each of the maps based on the CCC of the 4 different regression models:

```{r ensemble}
# We can weight results with Rsquared
w <- perf$rsq / sum(perf$rsq)

# Make stack of weighted predictions and compute sum
l_maps <- list(res_cubist$map, res_gam$map, res_lm$map, res_rf$map, res_earth$map)
ens <- lapply(1:5, function(x) l_maps[[x]] * w[x]) %>%
  stack %>%
  sum
```

Ensemble modelling seem to work better when the models are not too correlated -- i.e. when they capture different facets of the data:

```{r mod_cor}
s_res <- stack(l_maps)
names(s_res) <- c('Cubist', 'GAM', 'Linear Model', 'Random Forest', 'MARS')
s_res %>%
  as.data.frame %>%
  na.exclude %>%
  cor
```

Here is the resulting map:

```{r ensemble_plot, fig.width=6, fig.height=6}
# Plot result
plot(ens, col = viridis(100), main = "CCC Weighted Average")
```
